# 学习型排序 文稿
排序算是计算机科学中最最基础的算法了。在数据库中也是常用的操作，包括对查询结果进行排序，作为连接的一部分或索引。
本文提出了一个新的方法，利用数据的经验CDF的学习模型的分布查询。



## 介绍
基于计数的排序算法，像基数排序，时间复杂度是O(wN)，w是key的长度，N是key的个数，对于小key通常是最快的算法。

基于比较的排序，像快速排序和归并排序，时间复杂度是O(NlogN)。

本文介绍一种机器学习赋能的排序算法。核心思想非常简单：对于要排序的keys A，先抽样，根据样本训练出一个CDF模型F，然后用F预测每个key的位置，得到了一个大致有序的输出数组，最后再对这个数据进行一次精确排序。

假设我们能训练出一个完美的经验CDF模型F，对于每一个key x经过F得到它的位置pos=F(x)*N，如果F是完美的，就意味着我们只需要O(N)的时间就可以进行一次排序。

几个挑战：
1. 最重要的一点，不可能建立一个完美的CDF模型。并且对CDF建模的最新的方法，尤其是神经网络，训练和执行起来都过于昂贵。
2. 就算已经有了一个完美的CDF模型，速度可能还是要慢于高度优化的基数排序算法，因为技术排序可以用完全的顺序写来实现，而这种原始的学习型排序会导致大量的随机写在把数据直接放到它有序的应有的位置上的时候。

本文给出了一个cache效率高的机器学习赋能的排序算法。下面来看具体是怎么实现的。

## 实现
一个完美的CDF模型可以一遍排序。如图一

但事实上不可能实现一个完美的CDF模型，尤其是用来建立模型的数据是抽样的。另外数据集中还可能有重复的数据，可能会导致多个key映射到同一个位置。

下面给出了一个原始的学习型排序的算法，它对于不精确的模型已经有了鲁棒性，然后解释它为什么还是不行。

### 用不精确的模型进行排序
重复的key会导致被映射到同一地址。一些模型无法保证单调性，导致输出会有小的错位。也就是说对于key a<b, 可能出现F(a)>F(b)的情况，导致输出不是有序的。这样的话最后就需要纠正这些错误。所以学习型算法需要解决以下问题：
1. 纠正非单调模型的错误
2. 处理key的冲突
3. 减少冲突

然后给了伪代码，解释

下面给出了一些解决冲突的方法，也是在哈希中经常用到的
1. 线性探测：随着数据的填入，探测越来越远，效率会越来越差
2. 链地址法：需要跟踪指针和动态分配内存，影响效率
3. 溢出桶：所有冲突的都放到一个桶中，需要桶内排序和最后的合并

经过试验比较，发现溢出桶的性能最好。

对于不单调的模型，最后的排序用的是插入排序：1.错位的元素很少 2.离正确的位置很近

从算法1中可知，模型的质量决定了冲突的数量，而冲突又极大的影响性能。而冲突的多少取决于模型有多好的过拟合数据。比如，假设我们训练出的CDF模型和我们用来产生数据分布的模型是一致的，冲突率也会有1/e，36.7%，这是独立于分布的。这遵循了生日悖论，和哈希表冲突类似。但是过拟合还是能够减少大量冲突。但是如果为了减少训练成本基于抽样训练出这个一个模型，是不可能训练出这样一个完美的经验CDF的。

这里有一个权衡就是：过拟合（低冲突）和训练成本（抽样）

算法1类似哈希表，CDF模型F<sub>A</sub>是哈希表中保序的哈希函数。 但是就算是零开销的CDF模型，算法1还是比基数排序慢。主要原因在于算法1的随机的和不可预测的数组访问会验证影响CPU cache和TLB的局部性，导致大量的stall。而cache优化的基数排序是内存访问优化的，主要是连续的内存访问。

### Cache优化的学习型排序
idea来自于cache优化的基数排序。
基本思想如图4：
把位置的预测转换成预测属于哪个桶，来减少误差。

1.为了cache效率，从大桶开始递归的划分。在每次迭代的过程中通过合理的选择fanout，可以保证每个桶至少有一个cache line可以放进cache中，来把内存访问模式转换成顺序访问。这个递归划分的过程桶足够小到t。
2. 当桶足够小之后，用CDF模型来预测每个元素在桶中的精确的位置。
3. 然后就得到了有序的桶，把它们合并入到有序数组中。如果用的是不单调的模型，还要用插入排序纠正排序错误。
4. 桶的容量固定可以最小化内存动态分配的成本。如果桶满了，把剩下的放到单独的溢出桶中。在最后一步会对溢出桶排序然后合并。如果模型能够把数据比较均匀分到桶中，这一步的开销就很低。
   
算法2给出了伪代码

### 优化
+ 批量处理：先用模型获取批量数据所有的预测索引，然后放到预测的桶中。批处理可以维护缓存局部性。
+ 增加桶的大小，来减少溢出桶中溢出元素的数量
+ 

### CDF模型的选择
不依赖特定的模型，要求训练快，推理快。
KDE，神经网络和完美保序的哈希函数在训练和执行上都太贵了。或者可以采用直方图作为CDF模型，但是直方图排序已经有人提出了，并且粗粒度的直方图预测不准确，细粒度的直方图增加了直方图本身的遍历时间。

这里使用的是递归索引模型RMI。

推理和训练都给了伪代码。

然后是如何训练出一个线性模型，一种方法是用MSE损失函数的单变量线性回归闭式解，但是可能出现的一种情况会导致底层两个相邻的CDF模型可能会预测重叠范围的值，因此不能保证最终的预测是单调的，需要额外的插入排序。一种强制CDF模型为单调的方法是对每个叶子模型的预测范围进行边界检查，但是这样每个元素的预测都需要额外的分支指令。

最终采用的是单调性更好的线性样条拟合来训练模型，计算成本更低，只要在训练集在最大最小值上拟合一条线就行了。从单个模型的角度来看，这样的线条拟合精度是非常差的，但是考虑到这里是把线性模型作为RMI的一部分，即使对于高度偏斜的分布，总体上也能保证CDF的良好近似水平。优势：1.比线性回归闭式训练时间少2.7倍 2.在插入排序减少35%的数据交换。

## 算法分析

### 参数分析

fanout：大的fanout可以提升精度，但是fanout越多，更可能发生cache miss。